"""
ğŸ§  Self-Organizing Map (SOM) Implementation
==========================================

Author: Benedict Chen (benedict@benedictchen.com)

ğŸ’ Support This Work: https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&hosted_button_id=WXQKYYKPHWXHS

Developing high-quality, research-backed software takes countless hours of study, implementation, 
testing, and documentation. Your support - whether a little or a LOT - makes this work possible and is 
deeply appreciated. 

ğŸ¯ Help support continued research! Buy me a coffee â˜•, beer ğŸº, or lamborghini ğŸï¸

ğŸ’– Please consider recurring donations to fully support the work based on how much this module impacts your life or work!

Based on: Kohonen (1982) "Self-Organized Formation of Topologically Correct Feature Maps"

ğŸ¯ ELI5 Summary:
Think of a SOM like a smart map that learns to organize information by neighborhood.
If you show it pictures of animals, it will automatically group similar animals together
on a 2D grid, with cats near dogs, birds near each other, etc. No supervision needed!

ğŸ”¬ Research Background:
========================
Teuvo Kohonen's 1982 paper introduced a revolutionary unsupervised learning algorithm
that models how the brain organizes sensory information. The key insight: neurons
compete for inputs (winner-takes-all) while cooperating through neighborhood functions.
This creates topologically organized feature maps that preserve input space structure.

The SOM algorithm revolutionized:
- Data visualization (high-dimensional â†’ 2D maps)
- Clustering and classification
- Understanding cortical map formation
- Vector quantization and data compression

ğŸ—ï¸ Architecture:
================
Input Layer          SOM Grid (2D)           Output
-----------          --------------          ------
   ğŸ”µ                    ğŸŸ¦ğŸŸ¦ğŸŸ¦              
   ğŸ”µ        â†’          ğŸŸ¦ğŸŸ¨ğŸŸ¦          â†’    Clusters
   ğŸ”µ                    ğŸŸ¦ğŸŸ¦ğŸŸ¦              

Algorithm Flow:
1. ğŸ† Competition: Find Best Matching Unit (BMU) - neuron closest to input
2. ğŸ¤ Cooperation: Define neighborhood around BMU using distance functions  
3. ğŸ“š Adaptation: Update BMU and neighbors toward input (Hebbian learning)

Mathematical Framework:
- BMU: c = argmin_i ||x(t) - w_i(t)||
- Neighborhood: h_ci(t) = Î±(t) Ã— exp(-||r_c - r_i||Â²/2ÏƒÂ²(t))
- Weight update: w_i(t+1) = w_i(t) + h_ci(t)[x(t) - w_i(t)]

ğŸš€ Key Innovation: Unsupervised topological preservation
Revolutionary Impact: First algorithm to model biological cortical map formation

âš¡ Configurable Options:
=======================
âœ¨ Neighborhood Functions:
  - gaussian: exp(-dÂ²/2ÏƒÂ²) [default - smooth, biological]
  - mexican_hat: center-surround activation pattern
  - rectangular: binary step function neighborhood  
  - linear_decay: linear decrease within radius

âœ¨ Parameter Schedules:
  - exponential: Î·(t) = Î·â‚€ Ã— exp(-t/Ï„) [default - fast early learning]
  - linear: Î·(t) = Î·â‚€ Ã— (1 - t/T) [steady decay]
  - inverse_time: Î·(t) = Î·â‚€ / (1 + t/Ï„) [slow asymptotic decay]
  - power_law: Î·(t) = Î·â‚€ Ã— (tâ‚€/t)^Î± [scale-invariant decay]

ğŸ¨ ASCII Diagram - SOM Learning Process:
========================================
Initial (Random):        After Training:
    ğŸ”´ğŸŸ¢ğŸ”µ                  ğŸ”´ğŸ”´ğŸ”´
    ğŸŸ¡âš«ğŸŸ¤         â†’         ğŸŸ¢ğŸŸ¢ğŸŸ¢  
    ğŸŸ âšªğŸŸ£                  ğŸ”µğŸ”µğŸ”µ
    
Input: [0.9, 0.1, 0.1] â†’ Finds red cluster
BMU at (0,0), updates neighborhood:
    ğŸ”´â† BMU (winner)
    ğŸŸ¢â† Neighbor (cooperates)  
    ğŸ”µâ† Distant (no update)

ğŸ“š Usage Example:
================
```python
from self_organizing_maps import SelfOrganizingMap

# Create SOM with custom configuration
som = SelfOrganizingMap(
    map_size=(15, 15),              # 15Ã—15 neuron grid
    input_dim=3,                    # 3D input vectors
    neighborhood_function='gaussian', # Smooth neighborhoods
    parameter_schedule='exponential' # Fast early learning
)

# Train on your data
data = load_your_data()  # Shape: (n_samples, 3)
som.train(data, n_iterations=1000)

# Visualize results
som.visualize_map(data)

# Map new inputs
new_point = [0.5, 0.3, 0.7]
grid_position = som.map_input(new_point)
print(f"Input maps to grid position: {grid_position}")
```

ğŸ¯ Applications:
===============
- ğŸ“Š Data Visualization: High-dimensional data â†’ 2D maps
- ğŸ¯ Clustering: Unsupervised pattern discovery
- ğŸ§  Neuroscience: Understanding cortical organization
- ğŸ¨ Image Processing: Color quantization, compression
- ğŸ“ˆ Finance: Market segmentation, risk analysis
- ğŸ”Š Audio: Speech recognition, music analysis
"""

import numpy as np
import matplotlib.pyplot as plt
from typing import Dict, List, Tuple, Union, Optional, Any, Callable
from dataclasses import dataclass, field
import warnings
warnings.filterwarnings('ignore')


@dataclass
class SOMNeuron:
    """
    ğŸ§  SOM Neuron: Individual processing unit in the Self-Organizing Map
    
    ğŸ¯ ELI5: Like a brain cell that remembers what patterns it likes best.
    Each neuron has a "favorite pattern" (weight_vector) and remembers its
    location on the map (position) and how active it's been (activation_history).
    
    ğŸ”¬ Technical Details:
    Each neuron in the SOM grid maintains:
    - Spatial position (i, j) coordinates on the 2D lattice
    - Weight vector w_i representing its prototype/template
    - Activation history for analysis and visualization
    
    The neuron competes with others to respond to inputs by calculating
    the Euclidean distance: d = ||x - w_i|| where x is the input vector.
    The neuron with minimum distance becomes the Best Matching Unit (BMU).
    
    ğŸ“Š Attributes:
    - position: (i, j) grid coordinates - where this neuron sits on the map
    - weight_vector: w_i âˆˆ â„áµˆ - this neuron's learned feature template  
